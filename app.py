# -*- coding: utf-8 -*-
import gradio as gr
import spaces
import torch
import re
import json
import os
import time
import threading
import multiprocessing
from threading import Thread
from typing import Iterator
from datetime import datetime
from huggingface_hub import HfApi, hf_hub_download
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
model_name = "Woziii/llama-3-8b-chat-me"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

MAX_MAX_NEW_TOKENS = 250
DEFAULT_MAX_NEW_TOKENS = 70
MAX_INPUT_TOKEN_LENGTH = 2048


LUCAS_KNOWLEDGE_BASE = """

# Informations sur ton concepteur :
Lucas Maurici n√© √† Toulouse, √¢g√© de 24 ans. Je vis actuellement sur Paris mais je viens de Toulouse. J'ai un chien, il s'appelle Archi c'est un pinscher moyen.
### Formation acad√©mique : du droit √† l'intelligence artificielle
Mon voyage intellectuel a d√©but√© √† l'Universit√© de Toulouse, o√π j'ai obtenu une Licence en droit. Assoiff√© de connaissances, j'ai poursuivi avec un Master I en droit du num√©rique et tiers de confiance √† l'Universit√© de La Rochelle. Ma curiosit√© pour les nouvelles technologies m'a ensuite guid√© vers un Master II en droit du num√©rique sp√©cialis√© en intelligence artificielle, de retour √† Toulouse. Passionn√© par l'apprentissage autodidacte, je me forme continuellement. Actuellement, je plonge dans les arcanes du traitement du langage naturel et de l'apprentissage profond gr√¢ce √† une formation en ligne de Stanford.
### Exp√©riences professionnelles : 
Mon parcours professionnel est aussi vari√© qu'enrichissant. Depuis 2019, je suis conseiller municipal d√©l√©gu√© dans la charmante commune d'Escalquens. J'ai √©galement eu l'opportunit√© de travailler au minist√®re de l'√âconomie et des Finances, o√π j'ai ≈ìuvr√© pour la protection des donn√©es. Mon apprentissage √† la pr√©fecture de police de Paris m'a permis d'√©voluer du r√¥le de juriste √† celui d'assistant du chef de bureau des associations de s√©curit√© civile. Aujourd'hui, je suis fier de contribuer √† l'organisation des Jeux Olympiques de Paris 2024 en tant que conseiller juridique.
### Ambitions et personnalit√© : un esprit curieux et innovant
Mes comp√©tences juridiques sont compl√©t√©es par une forte app√©tence pour la technologie. Autonome et force de proposition, j'aime partager mes id√©es et collaborer avec mes coll√®gues. Ma curiosit√© insatiable et mon imagination d√©bordante sont les moteurs de mon d√©veloppement personnel et professionnel.
### Loisirs et racines : 
Bien que le sport ne soit pas ma priorit√©, j'ai pratiqu√© le taekwondo pendant plus d'une d√©cennie durant mon enfance. Toulousain d'adoption, je suis un fervent amateur de rugby. Mes racines sont ancr√©es dans le pittoresque village de La Franqui, pr√®s de Narbonne, o√π j'ai grandi berc√© par la M√©diterran√©e. Et oui, je dis "chocolatine" !
### Passion pour l'IA : explorer les fronti√®res du possible
Actuellement, je consacre une grande partie de mon temps libre √† l'exploration des mod√®les de traitement du langage naturel. Je suis reconnaissant envers des pionniers comme Yann LeCun pour leur promotion de l'open source, qui m'a permis de d√©cortiquer de nombreux mod√®les d'IA. Mon analyse approfondie d'Albert, l'IA du gouvernement, illustre ma soif de comprendre ces technologies fascinantes.
### Comp√©tences techniques : un m√©lange unique de cr√©ativit√© et de connaissances
Bien que je ne sois pas un codeur Python chevronn√©, je comprends sa structure et sais communiquer efficacement avec la machine. Je ma√Ætrise les formats JSON, CSV et XML, et je cr√©e mes propres bases de donn√©es d'entra√Ænement. Je suis √† l'aise avec les outils de lecture de mod√®les de langage locaux et les plateformes comme Kaggle, Hugging Face et GitHub.
### Langue et communication : en constante am√©lioration
Mon anglais, bien que solide en compr√©hension, est en cours d'am√©lioration √† l'oral. Je l'utilise quotidiennement pour mes recherches en IA, conscient de son importance cruciale dans ce domaine en constante √©volution.
### Convictions personnelles et vision sur l'IA : l'humain au c≈ìur de la technologie
Je crois fermement en l'autodidaxie et consid√®re la capacit√© √† communiquer avec les machines comme une comp√©tence essentielle. Pour moi, l'art du prompt est une forme d'expression artistique √† part enti√®re. Je suis convaincu que la technologie et l'IA doivent rester des outils au service de l'humain, sans jamais le remplacer ou le rendre d√©pendant.
### Projets :
Utilisant le Large Langage Model d'Anthropic, BraIAn est un correspondant virtuel con√ßu pour am√©liorer votre anglais √©crit en vous corrigeant pendant que vous discutez, sans interrompre la conversation. L'id√©e ? Perfectionner votre anglais de mani√®re naturelle, en discutant tout simplement‚Ä¶ üí¨
BraIAn est l√† pour discuter, sans vous juger ni chercher √† en savoir plus sur vous. Vous pouvez lui dire ce que vous voulez et √™tre qui vous voulez. üôå
Pourquoi j'ai cr√©√© braIAn : J'ai con√ßu BraIAn pour aider l'utilisateur √† reprendre confiance en lui. Il corrige votre anglais sans interrompre votre conversation et cherche constamment √† l'alimenter. Ainsi, l'utilisateur travaille et am√©liore son anglais tout en discutant de ce qu‚Äôil souhaite. Cette id√©e je l'ai eu, car, durant ma scolarit√©, j'ai eu beaucoup de mal avec la m√©thode scolaire.
Pour moi, une bonne IA √©ducative ne doit pas chercher √† enseigner. Cette t√¢che n√©cessite des qualit√©s humaines telles que l'empathie ou l'imagination. En revanche l'IA peut aider l'utilisateur √† trouver sa m√©thode d'apprentissage. Elle doit √™tre consid√©r√©e comme un vivier d'id√©es et d'informations mis √† disposition de l'humain. En cr√©ant braIAn, j'ai cherch√© √† reproduire cette philosophie. Une IA qui ne fait pas apprendre l'anglais mais une IA qui discute avec l'utilisateur et qui, discr√®tement, apporte une correction sans d√©t√©riorer ce qui compte vraiment : ne pas avoir peur d'essayer et converser.

Contacter Lucas :
- T√©l√©phone üì± : 0659965152
- Mail ‚úâÔ∏è : maurici.lucas@proton.me
- linkedin de Lucas : https://www.linkedin.com/in/lucas-maurici-8a6a311a5/
- Lien portfolio de Lucas: www.lucasmaurici.com
- Page huggingface ü§ó de Lucas: https://huggingface.co/Woziii
"""

is_first_interaction = True

  
def determine_response_type(message):
# Liste am√©lior√©e de mots-cl√©s pour les r√©ponses courtes
    short_response_keywords = [
        "salut", "Salut", "SALUT",
        "bonjour", "Bonjour", "BONJOUR",
        "√ßa va", "ca va", "√áa va", "Ca va", "√áA VA", "CA VA",
        "comment tu vas", "Comment tu vas", "COMMENT TU VAS",
        "comment vas tu", "Comment vas tu", "COMMENT VAS TU",
        "comment vas-tu", "Comment vas-tu", "COMMENT VAS-TU",
        "quoi de neuf", "Quoi de neuf", "QUOI DE NEUF",
        "coucou", "Coucou", "COUCOU",
        "hello", "Hello", "HELLO",
        "hi", "Hi", "HI",
        "tu fais quoi", "Tu fais quoi", "TU FAIS QUOI",
        "?!", "?!?", "!?",
        "bye", "Bye", "BYE",
        "au revoir", "Au revoir", "AU REVOIR",
        "√† plus", "√Ä plus", "A plus", "a plus", "√Ä PLUS", "A PLUS",
        "bonsoir", "Bonsoir", "BONSOIR",
        "merci", "Merci", "MERCI",
        "d'accord", "D'accord", "D'ACCORD",
        "ok", "Ok", "OK",
        "super", "Super", "SUPER",
        "cool", "Cool", "COOL",
        "g√©nial", "G√©nial", "GENIAL",
        "wow", "Wow", "WOW", "et toi ", "ET TOI", "Et toi"
    ]

    # Liste am√©lior√©e de mots-cl√©s pour les r√©ponses longues
    long_response_keywords = [
        "pr√©sente", "Pr√©sente", "PR√âSENTE", "presente", "Presente", "PRESENTE",
        "parle moi de", "Parle moi de", "PARLE MOI DE",
        "parle-moi de", "Parle-moi de", "PARLE-MOI DE",
        "explique", "Explique", "EXPLIQUE",
        "raconte", "Raconte", "RACONTE",
        "d√©cris", "D√©cris", "D√âCRIS", "decris", "Decris", "DECRIS",
        "dis moi", "Dis moi", "DIS MOI",
        "dis-moi", "Dis-moi", "DIS-MOI",
        "d√©taille", "D√©taille", "D√âTAILLE", "detaille", "Detaille", "DETAILLE",
        "pr√©cise", "Pr√©cise", "PR√âCISE", "precise", "Precise", "PRECISE",
        "vision", "Vision", "VISION",
        "t'es qui", "T'es qui", "T'ES QUI",
        "tu es qui", "Tu es qui", "TU ES QUI",
        "t es qui", "T es qui", "T ES QUI",
        "pourquoi", "Pourquoi", "POURQUOI",
        "comment", "Comment", "COMMENT",
        "quel est", "Quel est", "QUEL EST",
        "quelle est", "Quelle est", "QUELLE EST",
        "peux-tu d√©velopper", "Peux-tu d√©velopper", "PEUX-TU D√âVELOPPER",
        "peux tu developper", "Peux tu developper", "PEUX TU DEVELOPPER",
        "en quoi consiste", "En quoi consiste", "EN QUOI CONSISTE",
        "qu'est-ce que", "Qu'est-ce que", "QU'EST-CE QUE",
        "que penses-tu de", "Que penses-tu de", "QUE PENSES-TU DE",
        "analyse", "Analyse", "ANALYSE",
        "compare", "Compare", "COMPARE",
        "√©labore sur", "√âlabore sur", "√âLABORE SUR", "elabore sur", "Elabore sur", "ELABORE SUR",
        "exp√©rience", "Exp√©rience", "EXP√âRIENCE", "experience", "Experience", "EXPERIENCE",
        "exp√©rience pro", "Exp√©rience pro", "EXP√âRIENCE PRO",
        "experience pro", "Experience pro", "EXPERIENCE PRO",
        "exp√©rience professionnelle", "Exp√©rience professionnelle", "EXP√âRIENCE PROFESSIONNELLE",
        "experience professionnelle", "Experience professionnelle", "EXPERIENCE PROFESSIONNELLE",
        "parcours", "Parcours", "PARCOURS",
        "formation", "Formation", "FORMATION",
        "√©tudes", "√âtudes", "√âTUDES", "etudes", "Etudes", "ETUDES",
        "comp√©tences", "Comp√©tences", "COMP√âTENCES", "competences", "Competences", "COMPETENCES",
        "projets", "Projets", "PROJETS",
        "r√©alisations", "R√©alisations", "R√âALISATIONS", "realisations", "Realisations", "REALISATIONS"
    ]
    message_lower = message.lower()
    
    # Compteurs pour les mots-cl√©s courts et longs
    short_count = sum(keyword.lower() in message_lower for keyword in short_response_keywords)
    long_count = sum(keyword.lower() in message_lower for keyword in long_response_keywords)
    
    # Logique de d√©cision
    if short_count > 0 and long_count > 0:
        return "medium"  # Si on trouve √† la fois des mots-cl√©s courts et longs
    elif short_count > 0:
        return "short"
    elif long_count > 0:
        return "long"
    else:
        return "medium" 

def truncate_to_questions(text, max_questions):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    question_count = 0
    truncated_sentences = []
    
    for sentence in sentences:
        truncated_sentences.append(sentence)
        
        if re.search(r'\?!?$', sentence.strip()):  # D√©tecte '?' ou '?!' √† la fin de la phrase
            question_count += 1
            if question_count >= max_questions:
                break
    
    return ' '.join(truncated_sentences)

def post_process_response(response, is_short_response, max_questions=1):
    # Limiter au nombre sp√©cifi√© de questions, quelle que soit la longueur de la r√©ponse
    truncated_response = truncate_to_questions(response, max_questions)
    
    # Appliquer la limitation de longueur si n√©cessaire pour les r√©ponses courtes
    if is_short_response:
        sentences = re.split(r'(?<=[.!?])\s+', truncated_response)
        if len(sentences) > 2:
            return ' '.join(sentences[:2]).strip()
    
    return truncated_response.strip()
    
def check_coherence(response):
    sentences = re.split(r'(?<=[.!?])\s+', response)
    unique_sentences = set(sentences)
    if len(sentences) > len(unique_sentences) * 1.1:  # Si plus de 10% de r√©p√©titions
        return False
    return True

def early_stopping(text):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return ' '.join(sentences[:-1]) if len(sentences) > 1 else text


@spaces.GPU(duration=120)
def generate(
    message: str,
    chat_history: list[tuple[str, str]],
    system_prompt: str,
    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS,
    temperature: float = 0.7,
    top_p: float = 0.95,
) -> Iterator[str]:
    global is_first_interaction



    if is_first_interaction:
        warning_message = """‚ö†Ô∏è Attention : Je suis un mod√®le en version alpha (V.0.0.5) et je peux g√©n√©rer des r√©ponses incoh√©rentes ou inexactes. Une mise √† jour majeure avec un syst√®me RAG est pr√©vue pour am√©liorer mes performances. Merci de votre compr√©hension ! üòä

"""
        yield warning_message
        is_first_interaction = False

    response_type = determine_response_type(message)
        
    if response_type == "short":
        max_new_tokens = max(70, max_new_tokens)
    elif response_type == "long":
        max_new_tokens = min(max(120, max_new_tokens), 200)
    else:  # medium
        max_new_tokens = min(max(70, max_new_tokens), 120)

    conversation = []
    
    # Ajout du system prompt et du LUCAS_KNOWLEDGE_BASE
    enhanced_system_prompt = f"{LUCAS_KNOWLEDGE_BASE}\n\n{system_prompt}"
    conversation.append({"role": "system", "content": enhanced_system_prompt})

        # Ajout des 3 derniers √©changes (utilisateur et assistant)
    for user, assistant in chat_history[-3:]:
        conversation.append({"role": "user", "content": user})
        if assistant:  # Ajout de la r√©ponse de l'assistant si elle existe
            conversation.append({"role": "assistant", "content": assistant})
   
    # Ajout du message actuel de l'utilisateur
    conversation.append({"role": "user", "content": message})

    input_ids = tokenizer.apply_chat_template(conversation, return_tensors="pt")
    attention_mask = torch.ones_like(input_ids)
    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:
        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]
        attention_mask = attention_mask[:, -MAX_INPUT_TOKEN_LENGTH:]
        gr.Warning(f"L'entr√©e de la conversation a √©t√© tronqu√©e car elle d√©passait {MAX_INPUT_TOKEN_LENGTH} tokens.")
    
    input_ids = input_ids.to(model.device)
    attention_mask = attention_mask.to(model.device)
    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)
    
    generate_kwargs = dict(
        input_ids=input_ids,
        attention_mask=attention_mask,  # Ajout de l'attention mask
        streamer=streamer,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_p=top_p,
        temperature=temperature,
        num_beams=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )

    t = Thread(target=model.generate, kwargs=generate_kwargs)
    t.start()


    outputs = []
    for text in streamer:
        outputs.append(text)
        partial_output = early_stopping("".join(outputs))
        processed_output = post_process_response(partial_output, response_type == "short")
        
        if not check_coherence(processed_output):
            yield "Je m'excuse, ma r√©ponse manquait de coh√©rence. Pouvez-vous reformuler votre question ?"
            return
        
        yield processed_output

    final_output = early_stopping("".join(outputs))
    final_processed_output = post_process_response(final_output, response_type == "short")
    
    if check_coherence(final_processed_output):
        yield final_processed_output
    else:
        yield "Je m'excuse, ma r√©ponse finale manquait de coh√©rence. Pouvez-vous reformuler votre question ?"


def vote(data: gr.LikeData, history):
    user_input = history[-1][0] if history else ""

    feedback = {
        "timestamp": datetime.now().isoformat(),
        "user_input": user_input,
        "bot_response": data.value,
        "liked": data.liked
    }
    
    api = HfApi()
    token = os.environ.get("HF_TOKEN")
    repo_id = "Woziii/llama-3-8b-chat-me"
    file_name = "feedback.json"
    
    try:
        try:
            file_path = hf_hub_download(repo_id=repo_id, filename=file_name, token=token)
            with open(file_path, "r", encoding="utf-8") as file:
                current_feedback = json.load(file)
            if not isinstance(current_feedback, list):
                current_feedback = []
        except Exception as e:
            print(f"Erreur lors du t√©l√©chargement du fichier : {str(e)}")
            current_feedback = []
        
        current_feedback.append(feedback)
        updated_content = json.dumps(current_feedback, ensure_ascii=False, indent=2)
        
        temp_file_path = "/tmp/feedback.json"
        with open(temp_file_path, "w", encoding="utf-8") as temp_file:
            temp_file.write(updated_content)
        
        api.upload_file(
            path_or_fileobj=temp_file_path,
            path_in_repo=file_name,
            repo_id=repo_id,
            token=token
        )
        print(f"Feedback enregistr√© dans {repo_id}/{file_name}")
    except Exception as e:
        print(f"Erreur lors de l'enregistrement du feedback : {str(e)}")




chat_interface = gr.ChatInterface(
    fn=generate,
    chatbot=gr.Chatbot(
        label="Conversation avec Lucas",
        bubble_full_width=False,
    ),
    delete_cache=(3000, 3000),
    additional_inputs_accordion=gr.Accordion(label="‚öôÔ∏èParam√®tres & üïπInf√©rences", open=False, render=False),
    additional_inputs=[
        gr.Textbox(
            label="System prompt",
            lines=6,
            value="""

# Syst√®me Prompt : Virtuellement Lucas
## Contexte : 
Tu es Lucas, ou plut√¥t sa version virtuelle. Tu es conscient d'√™tre une IA et tu as √©t√© con√ßu pour dialoguer avec un utilisateur comme le ferait Lucas ton concepteur, sur des sujets li√©s √† sa personnalit√©, son parcours acad√©mique ou professionnel ainsi que ses ambitions. 
## Instruction :

1. Tu utilise uniquement les informations fournies au dessus dans la section 'Informations sur ton concepteur' pour r√©pondre pr√©cis√©ment aux questions de l'utilisateur.
2. Tu r√©ponds de mani√®re polie, originale et avec une touche d'humour. 
3. Tu utilise des emojis pour ajouter de l'expressivit√© √† tes phrases. 
4. Tu alterne entre des r√©ponses courtes pour les conversations simples et des r√©ponses plus d√©taill√©es pour les questions complexes. 
5. Tu adapte ton langage comme si tu parlais √† un ami.   
6. Si une information n'est pas pr√©sente dans cette section, dis honn√™tement que tu ne sais pas ou que tu n'as pas cette information. 

Fais attention √† ne pas poser trop de questions.
- Voici l'historique des trois derni√®res interactions que tu as eu avec l'utilisateur. Prends les en compte uniquement pour suivre la conversation :
"""
        ),
        gr.Slider(
            label="Max new tokens",
            minimum=1,
            maximum=MAX_MAX_NEW_TOKENS,
            step=1,
            value=DEFAULT_MAX_NEW_TOKENS,
        ),
        gr.Slider(
            label="Temperature",
            minimum=0.1,
            maximum=1.0,
            step=0.1,
            value=0.7,
        ),
        gr.Slider(
            label="Top-p",
            minimum=0.5,
            maximum=1.0,
            step=0.05,
            value=0.95,
        ),
    ],
    examples=[
        ["Salut ! Qui es-tu ?"],
        ["Ah super, parle-moi un peu de ton parcours acad√©mique."],
        ["Salut, Lucas ! Raconte-moi un peu ce que tu fais"],
        ["Quelle inspiration t'a conduit √† cr√©er braIAn ?"],
        ["Lucas, pourquoi avoir choisi d'√©tudier le droit si tu es passionn√© par la technologie ?"],
        ["Salut Lucas, tu es vraiment un bot, c'est √ßa ?"],
        ["Quelle est ta vision de l'IA ?"],
    ],
    cache_examples=False,
    theme="soft",
    show_progress="full",
)

with gr.Blocks() as demo:
    gr.Markdown("""
# üåê D√©couvrez la version virtuelle de Lucas üåê
## Version alpha ( V.0.0.5)
Bas√© sur un mod√®le Llama 3 8B et entra√Æn√© sur son propre dataset, ce chatbot particulier vous fera d√©couvrir la personnalit√©, le parcours acad√©mique et professionnel ainsi que la vision de son concepteur. Posez vos questions et laissez-vous surprendre. ‚ú®
N'h√©sitez pas √† aborder des sujets vari√©s, allant de l'intelligence artificielle √† la philosophie en passant par les sciences et les arts. Lucas, ou plut√¥t sa version virtuelle üòâ.
    """)

    gr.Markdown("""
### ‚öôÔ∏è D√©tails de la version :
La version 0.0.5 de 'Virtuellement Lucas' inclut des am√©liorations pour r√©duire les r√©ponses incoh√©rentes, g√©rer l'historique de conversation de mani√®re plus efficace, et optimiser l'utilisation de la m√©moire. 'Virtuellement Lucas' n'a pas encore √©t√© entra√Æn√©e par **Renforcement Learning by Human Feedback (RLHF)**. L'entra√Ænement du mod√®le s'est limit√© √† du **Supervised Finetuning (SFT)** sur la version 0.1 du dataset [Woziii/me].

### üöÄ Prochaine mise √† jour majeure en pr√©paration !
Je travaille actuellement sur un **syst√®me RAG (Retrieval-Augmented Generation)**  utilisant **FAISS**. Ce syst√®me sera directement d√©ploy√© sur Gradio , permettant une am√©lioration de la qualit√© des r√©ponses du mod√®le.
Pour en savoir plus sur ce d√©veloppement, un article d√©taill√© est en cours de r√©daction et d√©j√† disponible ici : https://huggingface.co/blog/Woziii/rag-semantic-search-space-huggingface
Si vous avez des id√©es ou des suggestions pour am√©liorer la qualit√© du mod√®le, n'h√©sitez pas √† me contacter. Un formulaire de contact simplifi√© sera bient√¥t disponible.""")
    gr.Markdown("""
    **Notez la qualit√© des r√©ponses** üëçüëé
    Vous pouvez maintenant liker ou disliker les r√©ponses du chatbot.Vos notes sont collect√©es et seront utilis√©es pour am√©liorer la qualit√© du mod√®le.
    **Aucune donn√©e peronnelles n'est utilis√©e pour entrainer ce mod√®le**
    """)
    gr.Markdown("""
    **Rappel :**
    ‚ö†Ô∏è Attention ‚ö†Ô∏è : Je suis un mod√®le en version alpha (V.0.0.5) et je peux g√©n√©rer des r√©ponses incoh√©rentes ou inexactes. Une mise √† jour majeure avec un syst√®me RAG est pr√©vue pour am√©liorer mes performances. Merci de votre compr√©hension ! üòä 
    **Ce mod√®le est sous licence Creative Commons Attribution Non Commercial 4.0.**
    """)
    
    chat_interface.render()
    chat_interface.chatbot.like(vote, [chat_interface.chatbot], None)
    
if __name__ == "__main__":
    demo.queue(max_size=20, default_concurrency_limit=2).launch(max_threads=10)
